#data related config
data:
  dataset: lfpaper2keywords
  is_lf_data: True
  augment_label_data: True
  use_filter_eval: False
  num_labels: 8623847
  max_len: 128
  num_workers: 4
  batch_size: 128
  test_batch_size: 128
  dtype: float32
  label_sort: False
  tokenized_loading: False

#model related config
model:
  encoder:
    encoder_model: "sentence-transformers/paraphrase-distilroberta-base-v2" #['sentence-transformers/all-roberta-large-v1','bert-base-uncased',
                                                  #'sentence-transformers/msmarco-distilbert-base-v4',sentence-transformers/all-roberta-large-v1]
    encoder_tokenizer: "${dataset.model.encoder.encoder_model}"
    encoder_ftr_dim: ${encoder_feature_size:${dataset.model.encoder.encoder_model}}
    pool_mode: 'last_hidden_avg' #[last_nhidden_conlast,last_hidden_avg]
    feature_layers: 1
    embed_dropout: 0.75
    use_torch_compile: True
    use_ngame_encoder_weights: False
    ngame_checkpoint: ./NGAME_ENCODERS/${dataset.data.dataset}/state_dict.pt
    dtype: bfloat16

  bottleneck:
    use_bottleneck_layer: False
    bottleneck_size: 64
    bottleneck_activation: relu
    dtype: float32

  xmc:
    layer: dense
    input_features: ${input_size_select:${dataset.model.bottleneck.use_bottleneck_layer},
                          ${dataset.model.bottleneck.bottleneck_size},${dataset.model.encoder.feature_layers},${dataset.model.encoder.encoder_ftr_dim}}   #set based on penultimate value in Encoder
    output_features: ${data.num_labels} #depends on num_labels in data
    use_torch_compile: False
    dtype: bfloat16
    implementation: fp8chunked  #auto selection of XMC class based on implementation type [chunked,]
    num_chunks: 8

#training related config
training:
  seed: 42

  precision:
    mode: custom  #[ purefp- uniform full precision, no amp or lower precision, custom-heterogeneous precision]
    purelp_dtype: bfloat16

  loss_fn: bce   # ['bce','squared_hinge'] # other loss currently doesn't support fused implementation
  epochs: 101
  training_steps: 1  #selected at runtime based on batch size and dataloader
  
  encoder:
    lr: 1.0e-5  # learning rate
    wd: 0.01   # weight decay
    optimizer: adam  # [adam,adamw,sgd]
    momentum: 0 #used with sgd
    lr_scheduler: CosineScheduleWithWarmup   #[MultiStepLR,CosineScheduleWithWarmup,ReduceLROnPlateau]
    warmup_steps: 5000
    grad_accum_step: 1
    implementation: optimi  #[optimi,pytorch,custom,adamwbf16]

  xmc:
    lr: 0.05  # learning rate
    wd: 0.0001   # weight decay
    optimizer: sgd  # [adam,adamw,sgd]
    momentum: 0  #used with sgd
    lr_scheduler: CosineScheduleWithWarmup   #[MultiStepLR,CosineScheduleWithWarmup,ReduceLROnPlateau]
    warmup_steps: 5000
    simulated_fp8: False

  bottleneck:
    lr: ${encoder_optimizer.lr}
    wd: ${encoder_optimizer.wd}
    
  FP8:
    fp8_embed: True
    use_scaled_mm: False
    use_fp8_encoder: False
    fp8_encoder_delayed_scaling: False
    head_ratio: 0.2
    using_head_kahan: False
    fp8_head_kahan: False
    debug_magnitude: False
    debug_data_path: "./"
    num_split: 4

  evaluation:
    running_evaluation: True
    train_evaluate: True
    train_evaluate_every: 10
    test_evaluate_every: 1
    A: 0.5  # for propensity calculation
    B: 0.4  # for propensity calculation
    eval_psp: True
    eval_recall: False
    eval_ndcg: False
    eval_psr: False

  verbose:
    show_iter: False  # print loss during training
    print_iter: 2000  # how often (iteration) to print
    use_wandb: False
    wandb_runname: none
    logging: True
    log_fname: log_Walso320k
    
  use_checkpoint: True  #whether to use automatic checkpoint
  checkpoint_file: lf2keywords
  load_checkpoint_file: None
  best_p1: 0.2  # to store the model above this performance in case of automatic checkpoint
